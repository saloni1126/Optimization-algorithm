{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNS4YPQjhD0Go1imhqmTFai"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["**1. Particle swarm Optimization using Squared Sine Activation function**"],"metadata":{"id":"dmY39HSrnQJh"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"4wBpa4XAZMCP"},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras.utils import to_categorical\n","import numpy as np\n","# Loading MNIST data set\n","mnist = tf.keras.datasets.mnist\n","(X_train, y_train), (X_test, y_test) = mnist.load_data()\n","\n","# Normalizing the pixel values to the range [0, 1]\n","X_train, X_test = X_train / 255.0, X_test / 255.0\n","\n","# Converting labels to categorical\n","y_train = to_categorical(y_train, 10)\n","y_test = to_categorical(y_test, 10)\n","# Custom Squared Sine Activation Function\n","class SquaredSineActivation(tf.keras.layers.Layer):\n","    def __init__(self, omega=1.0, **kwargs):\n","        super(SquaredSineActivation, self).__init__(**kwargs)\n","        self.omega = omega\n","\n","    def call(self, inputs):\n","        return tf.sin(self.omega * inputs) ** 2\n","\n","# CNN architecture with Squared Sine Activation\n","def create_cnn_model(activation_function):\n","    model = Sequential()\n","    model.add(Conv2D(32, (3, 3), padding='same', activation=activation_function, input_shape=(28, 28, 1)))\n","    model.add(MaxPooling2D(pool_size=(2, 2)))\n","    model.add(Flatten())\n","    model.add(Dense(128, activation=activation_function))\n","    model.add(Dense(10, activation='softmax'))\n","    return model\n","    # Compiling the model\n","    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","# Define the objective function for Particle Swarm Optimization\n","def objective_function(params):\n","    learning_rate = params[0]\n","    batch_size = int(params[1])\n","    epochs = int(params[2])\n","\n","    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n","                  loss='categorical_crossentropy',\n","                  metrics=['accuracy'])\n","\n","    history = model.fit(X_train, y_train, validation_data=(X_test, y_test),\n","                        batch_size=batch_size, epochs=epochs, verbose=0)\n","\n","    train_loss = history.history['loss'][-1]\n","    train_accuracy = history.history['accuracy'][-1]\n","\n","    return train_loss + 1 - train_accuracy  # PSO minimizes the objective function, so maximize accuracy.\n","# Define the search space for PSO\n","param_boundaries = [\n","    (1e-6, 1e-4),  # Learning rate\n","    (32, 50),     # Batch size\n","    (5, 10)        # Epochs\n","]\n","num_particles= 50\n","# Number of iterations\n","num_iterations = 50\n","# Particle Swarm Optimization\n","class ParticleSwarmOptimizer:\n","    def __init__(self, num_particles, objective_function, param_boundaries):\n","        self.num_particles = num_particles\n","        self.objective_function = objective_function\n","        self.param_boundaries = param_boundaries\n","\n","    def optimize(self, num_iterations):\n","        # Initialization\n","        particles = [self._initialize_particle() for _ in range(self.num_particles)]\n","        global_best_position = particles[0][0]\n","        global_best_fitness = particles[0][-1]\n","\n","        for iteration in range(num_iterations):\n","            for i in range(self.num_particles):\n","                # Update velocity and position\n","                new_velocity = self._update_velocity(particles[i][1], particles[i][2], particles[i][3],\n","                                                     particles[i][0], global_best_position)\n","                new_position = self._update_position(particles[i][0], new_velocity)\n","\n","                # Ensure the search space boundaries\n","                new_position = [self._clip(val, *self.param_boundaries[idx]) for idx, val in enumerate(new_position)]\n","\n","                # Update fitness\n","                new_fitness = self.objective_function(new_position)\n","\n","                # Update personal best\n","                if new_fitness < particles[i][-1]:\n","                    particles[i] = [new_position, *new_velocity, new_fitness]\n","\n","                # Update global best\n","                if new_fitness < global_best_fitness:\n","                    global_best_position = new_position\n","                    global_best_fitness = new_fitness\n","\n","        # Returning the best solution found\n","        return global_best_position\n","\n","    def _initialize_particle(self):\n","        position = [self._get_random_in_range(*boundaries) for boundaries in self.param_boundaries]\n","        velocity = [self._get_random_in_range(-1, 1) for _ in range(len(self.param_boundaries))]\n","        fitness = self.objective_function(position)\n","        return [position, *velocity, fitness]\n","\n","    def _update_velocity(self, w, c1, c2, current_position, global_best_position):\n","        inertia = w * np.array(current_position[1:])\n","        cognitive_component = c1 * self._get_random() * (np.array(current_position[3:]) - np.array(current_position[1:]))\n","        social_component = c2 * self._get_random() * (np.array(global_best_position) - np.array(current_position[1:]))\n","        new_velocity = inertia + cognitive_component + social_component\n","        return new_velocity.tolist()\n","\n","    def _update_position(self, current_position, velocity):\n","        new_position = np.array(current_position) + np.array([0] + velocity)\n","        return new_position.tolist()\n","\n","    @staticmethod\n","    def _get_random():\n","        return np.random.uniform(low=0, high=1)\n","\n","    @staticmethod\n","    def _get_random_in_range(min_val, max_val):\n","        return np.random.uniform(low=min_val, high=max_val)\n","\n","    @staticmethod\n","    def _clip(value, min_val, max_val):\n","        return max(min(value, max_val), min_val)\n","# Creating a Particle Swarm Optimizer object\n","pso = ParticleSwarmOptimizer(num_particles, objective_function, param_boundaries)\n","# Finding the optimal parameters\n","optimal_params = pso.optimize(num_iterations)\n","# Displaying the best parameters found\n","print(f\"Best parameters found by PSO: {optimal_params}\")\n","# Compiling the model with the best parameters\n","model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=optimal_params[0]),\n","              loss='categorical_crossentropy',\n","              metrics=['accuracy'])\n","# Training the model with the best parameters\n","history = model.fit(X_train, y_train, batch_size=int(optimal_params[1]), epochs=int(optimal_params[2]), verbose=1)\n","# Evaluating the model on the test set\n","test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n","print(f\"Test loss: {test_loss}, Test accuracy: {test_accuracy}\")"]},{"cell_type":"markdown","source":["**2. Particle swarm  Optimization using relu Activation function**"],"metadata":{"id":"cqe88-CAnUKE"}},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras.utils import to_categorical\n","# Loading MNIST data set\n","mnist = tf.keras.datasets.mnist\n","(X_train, y_train), (X_test, y_test) = mnist.load_data()\n","# Normalizing the pixel values to the range [0, 1]\n","X_train, X_test = X_train / 255.0, X_test / 255.0\n","# Converting labels to categorical\n","y_train = to_categorical(y_train, 10)\n","y_test = to_categorical(y_test, 10)\n","# Converting labels to categorical\n","y_train = to_categorical(y_train, 10)\n","y_test = to_categorical(y_test, 10)\n","# CNN architecture\n","model = Sequential()\n","model.add(Conv2D(32, (3, 3), padding='same', activation='relu', input_shape=(28, 28, 1)))\n","model.add(MaxPooling2D(pool_size=(2, 2)))\n","model.add(Flatten())\n","model.add(Dense(128, activation='relu'))\n","model.add(Dense(10, activation='softmax'))\n","# Compiling the model\n","model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","# Define the objective function for Particle Swarm Optimization\n","def objective_function(params):\n","    learning_rate = params[0]\n","    batch_size = int(params[1])\n","    epochs = int(params[2])\n","\n","    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n","                  loss='categorical_crossentropy',\n","                  metrics=['accuracy'])\n","\n","    history = model.fit(X_train, y_train, validation_data=(X_test, y_test),\n","                        batch_size=batch_size, epochs=epochs, verbose=0)\n","\n","    train_loss = history.history['loss'][-1]\n","    train_accuracy = history.history['accuracy'][-1]\n","\n","    return train_loss + 1 - train_accuracy  # PSO minimizes the objective function, so maximize accuracy.\n","# Define the search space for PSO\n","param_boundaries = [\n","    (1e-6, 1e-4),  # Learning rate\n","    (32, 50),     # Batch size\n","    (5, 10)        # Epochs\n","]\n","num_particles= 50\n","# Number of iterations\n","num_iterations = 50\n","# Particle Swarm Optimization\n","class ParticleSwarmOptimizer:\n","    def __init__(self, num_particles, objective_function, param_boundaries):\n","        self.num_particles = num_particles\n","        self.objective_function = objective_function\n","        self.param_boundaries = param_boundaries\n","\n","    def optimize(self, num_iterations):\n","        # Initialization\n","        particles = [self._initialize_particle() for _ in range(self.num_particles)]\n","        global_best_position = particles[0][0]\n","        global_best_fitness = particles[0][-1]\n","\n","        for iteration in range(num_iterations):\n","            for i in range(self.num_particles):\n","                # Update velocity and position\n","                new_velocity = self._update_velocity(particles[i][1], particles[i][2], particles[i][3],\n","                                                     particles[i][0], global_best_position)\n","                new_position = self._update_position(particles[i][0], new_velocity)\n","\n","                # Ensure the search space boundaries\n","                new_position = [self._clip(val, *self.param_boundaries[idx]) for idx, val in enumerate(new_position)]\n","\n","                # Update fitness\n","                new_fitness = self.objective_function(new_position)\n","\n","                # Update personal best\n","                if new_fitness < particles[i][-1]:\n","                    particles[i] = [new_position, *new_velocity, new_fitness]\n","\n","                # Update global best\n","                if new_fitness < global_best_fitness:\n","                    global_best_position = new_position\n","                    global_best_fitness = new_fitness\n","\n","        # Returning the best solution found\n","        return global_best_position\n","\n","    def _initialize_particle(self):\n","        position = [self._get_random_in_range(*boundaries) for boundaries in self.param_boundaries]\n","        velocity = [self._get_random_in_range(-1, 1) for _ in range(len(self.param_boundaries))]\n","        fitness = self.objective_function(position)\n","        return [position, *velocity, fitness]\n","\n","    def _update_velocity(self, w, c1, c2, current_position, global_best_position):\n","        inertia = w * np.array(current_position[1:])\n","        cognitive_component = c1 * self._get_random() * (np.array(current_position[3:]) - np.array(current_position[1:]))\n","        social_component = c2 * self._get_random() * (np.array(global_best_position) - np.array(current_position[1:]))\n","        new_velocity = inertia + cognitive_component + social_component\n","        return new_velocity.tolist()\n","\n","    def _update_position(self, current_position, velocity):\n","        new_position = np.array(current_position) + np.array([0] + velocity)\n","        return new_position.tolist()\n","\n","    @staticmethod\n","    def _get_random():\n","        return np.random.uniform(low=0, high=1)\n","\n","    @staticmethod\n","    def _get_random_in_range(min_val, max_val):\n","        return np.random.uniform(low=min_val, high=max_val)\n","\n","    @staticmethod\n","    def _clip(value, min_val, max_val):\n","        return max(min(value, max_val), min_val)\n","# Creating a Particle Swarm Optimizer object\n","pso = ParticleSwarmOptimizer(num_particles, objective_function, param_boundaries)\n","# Finding the optimal parameters\n","optimal_params = pso.optimize(num_iterations)\n","# Displaying the best parameters found\n","print(f\"Best parameters found by PSO: {optimal_params}\")\n","# Compiling the model with the best parameters\n","model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=optimal_params[0]),\n","              loss='categorical_crossentropy',\n","              metrics=['accuracy'])\n","# Training the model with the best parameters\n","history = model.fit(X_train, y_train, batch_size=int(optimal_params[1]), epochs=int(optimal_params[2]), verbose=1)\n","# Evaluating the model on the test set\n","test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n","print(f\"Test loss: {test_loss}, Test accuracy: {test_accuracy}\")"],"metadata":{"id":"NX52wTWPnUA-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**3. Particle swarm  Optimization using sigmoid Activation function**"],"metadata":{"id":"ISlFu7O0nT4C"}},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras.utils import to_categorical\n","# Loading MNIST data set\n","mnist = tf.keras.datasets.mnist\n","(X_train, y_train), (X_test, y_test) = mnist.load_data()\n","# Normalizing the pixel values to the range [0, 1]\n","X_train, X_test = X_train / 255.0, X_test / 255.0\n","# Converting labels to categorical\n","y_train = to_categorical(y_train, 10)\n","y_test = to_categorical(y_test, 10)\n","# Converting labels to categorical\n","y_train = to_categorical(y_train, 10)\n","y_test = to_categorical(y_test, 10)\n","# CNN architecture\n","model = Sequential()\n","model.add(Conv2D(32, (3, 3), padding='same', activation='sigmoid', input_shape=(28, 28, 1)))\n","model.add(MaxPooling2D(pool_size=(2, 2)))\n","model.add(Flatten())\n","model.add(Dense(128, activation='sigmoid'))\n","model.add(Dense(10, activation='softmax'))\n","# Compiling the model\n","model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","# Define the objective function for Particle Swarm Optimization\n","def objective_function(params):\n","    learning_rate = params[0]\n","    batch_size = int(params[1])\n","    epochs = int(params[2])\n","\n","    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n","                  loss='categorical_crossentropy',\n","                  metrics=['accuracy'])\n","\n","    history = model.fit(X_train, y_train, validation_data=(X_test, y_test),\n","                        batch_size=batch_size, epochs=epochs, verbose=0)\n","\n","    train_loss = history.history['loss'][-1]\n","    train_accuracy = history.history['accuracy'][-1]\n","\n","    return train_loss + 1 - train_accuracy  # PSO minimizes the objective function, so maximize accuracy.\n","# Define the search space for PSO\n","param_boundaries = [\n","    (1e-6, 1e-4),  # Learning rate\n","    (32, 50),     # Batch size\n","    (5, 10)        # Epochs\n","]\n","num_particles= 50\n","# Number of iterations\n","num_iterations = 50\n","# Particle Swarm Optimization\n","class ParticleSwarmOptimizer:\n","    def __init__(self, num_particles, objective_function, param_boundaries):\n","        self.num_particles = num_particles\n","        self.objective_function = objective_function\n","        self.param_boundaries = param_boundaries\n","\n","    def optimize(self, num_iterations):\n","        # Initialization\n","        particles = [self._initialize_particle() for _ in range(self.num_particles)]\n","        global_best_position = particles[0][0]\n","        global_best_fitness = particles[0][-1]\n","\n","        for iteration in range(num_iterations):\n","            for i in range(self.num_particles):\n","                # Update velocity and position\n","                new_velocity = self._update_velocity(particles[i][1], particles[i][2], particles[i][3],\n","                                                     particles[i][0], global_best_position)\n","                new_position = self._update_position(particles[i][0], new_velocity)\n","\n","                # Ensure the search space boundaries\n","                new_position = [self._clip(val, *self.param_boundaries[idx]) for idx, val in enumerate(new_position)]\n","\n","                # Update fitness\n","                new_fitness = self.objective_function(new_position)\n","\n","                # Update personal best\n","                if new_fitness < particles[i][-1]:\n","                    particles[i] = [new_position, *new_velocity, new_fitness]\n","\n","                # Update global best\n","                if new_fitness < global_best_fitness:\n","                    global_best_position = new_position\n","                    global_best_fitness = new_fitness\n","\n","        # Returning the best solution found\n","        return global_best_position\n","\n","    def _initialize_particle(self):\n","        position = [self._get_random_in_range(*boundaries) for boundaries in self.param_boundaries]\n","        velocity = [self._get_random_in_range(-1, 1) for _ in range(len(self.param_boundaries))]\n","        fitness = self.objective_function(position)\n","        return [position, *velocity, fitness]\n","\n","    def _update_velocity(self, w, c1, c2, current_position, global_best_position):\n","        inertia = w * np.array(current_position[1:])\n","        cognitive_component = c1 * self._get_random() * (np.array(current_position[3:]) - np.array(current_position[1:]))\n","        social_component = c2 * self._get_random() * (np.array(global_best_position) - np.array(current_position[1:]))\n","        new_velocity = inertia + cognitive_component + social_component\n","        return new_velocity.tolist()\n","\n","    def _update_position(self, current_position, velocity):\n","        new_position = np.array(current_position) + np.array([0] + velocity)\n","        return new_position.tolist()\n","\n","    @staticmethod\n","    def _get_random():\n","        return np.random.uniform(low=0, high=1)\n","\n","    @staticmethod\n","    def _get_random_in_range(min_val, max_val):\n","        return np.random.uniform(low=min_val, high=max_val)\n","\n","    @staticmethod\n","    def _clip(value, min_val, max_val):\n","        return max(min(value, max_val), min_val)\n","# Creating a Particle Swarm Optimizer object\n","pso = ParticleSwarmOptimizer(num_particles, objective_function, param_boundaries)\n","# Finding the optimal parameters\n","optimal_params = pso.optimize(num_iterations)\n","# Displaying the best parameters found\n","print(f\"Best parameters found by PSO: {optimal_params}\")\n","# Compiling the model with the best parameters\n","model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=optimal_params[0]),\n","              loss='categorical_crossentropy',\n","              metrics=['accuracy'])\n","# Training the model with the best parameters\n","history = model.fit(X_train, y_train, batch_size=int(optimal_params[1]), epochs=int(optimal_params[2]), verbose=1)\n","# Evaluating the model on the test set\n","test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n","print(f\"Test loss: {test_loss}, Test accuracy: {test_accuracy}\")"],"metadata":{"id":"jAveevUFnTtK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**4. Particle swarm  Optimization using tanh Activation function**"],"metadata":{"id":"09PP4aOknTkM"}},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras.utils import to_categorical\n","# Loading MNIST data set\n","mnist = tf.keras.datasets.mnist\n","(X_train, y_train), (X_test, y_test) = mnist.load_data()\n","# Normalizing the pixel values to the range [0, 1]\n","X_train, X_test = X_train / 255.0, X_test / 255.0\n","# Converting labels to categorical\n","y_train = to_categorical(y_train, 10)\n","y_test = to_categorical(y_test, 10)\n","# Converting labels to categorical\n","y_train = to_categorical(y_train, 10)\n","y_test = to_categorical(y_test, 10)\n","# CNN architecture\n","model = Sequential()\n","model.add(Conv2D(32, (3, 3), padding='same', activation='tanh', input_shape=(28, 28, 1)))\n","model.add(MaxPooling2D(pool_size=(2, 2)))\n","model.add(Flatten())\n","model.add(Dense(128, activation='tanh'))\n","model.add(Dense(10, activation='softmax'))\n","# Compiling the model\n","model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","# Define the objective function for Particle Swarm Optimization\n","def objective_function(params):\n","    learning_rate = params[0]\n","    batch_size = int(params[1])\n","    epochs = int(params[2])\n","\n","    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n","                  loss='categorical_crossentropy',\n","                  metrics=['accuracy'])\n","\n","    history = model.fit(X_train, y_train, validation_data=(X_test, y_test),\n","                        batch_size=batch_size, epochs=epochs, verbose=0)\n","\n","    train_loss = history.history['loss'][-1]\n","    train_accuracy = history.history['accuracy'][-1]\n","\n","    return train_loss + 1 - train_accuracy  # PSO minimizes the objective function, so maximize accuracy.\n","# Define the search space for PSO\n","param_boundaries = [\n","    (1e-6, 1e-4),  # Learning rate\n","    (32, 50),     # Batch size\n","    (5, 10)        # Epochs\n","]\n","num_particles= 50\n","# Number of iterations\n","num_iterations = 50\n","# Particle Swarm Optimization\n","class ParticleSwarmOptimizer:\n","    def __init__(self, num_particles, objective_function, param_boundaries):\n","        self.num_particles = num_particles\n","        self.objective_function = objective_function\n","        self.param_boundaries = param_boundaries\n","\n","    def optimize(self, num_iterations):\n","        # Initialization\n","        particles = [self._initialize_particle() for _ in range(self.num_particles)]\n","        global_best_position = particles[0][0]\n","        global_best_fitness = particles[0][-1]\n","\n","        for iteration in range(num_iterations):\n","            for i in range(self.num_particles):\n","                # Update velocity and position\n","                new_velocity = self._update_velocity(particles[i][1], particles[i][2], particles[i][3],\n","                                                     particles[i][0], global_best_position)\n","                new_position = self._update_position(particles[i][0], new_velocity)\n","\n","                # Ensure the search space boundaries\n","                new_position = [self._clip(val, *self.param_boundaries[idx]) for idx, val in enumerate(new_position)]\n","\n","                # Update fitness\n","                new_fitness = self.objective_function(new_position)\n","\n","                # Update personal best\n","                if new_fitness < particles[i][-1]:\n","                    particles[i] = [new_position, *new_velocity, new_fitness]\n","\n","                # Update global best\n","                if new_fitness < global_best_fitness:\n","                    global_best_position = new_position\n","                    global_best_fitness = new_fitness\n","\n","        # Returning the best solution found\n","        return global_best_position\n","\n","    def _initialize_particle(self):\n","        position = [self._get_random_in_range(*boundaries) for boundaries in self.param_boundaries]\n","        velocity = [self._get_random_in_range(-1, 1) for _ in range(len(self.param_boundaries))]\n","        fitness = self.objective_function(position)\n","        return [position, *velocity, fitness]\n","\n","    def _update_velocity(self, w, c1, c2, current_position, global_best_position):\n","        inertia = w * np.array(current_position[1:])\n","        cognitive_component = c1 * self._get_random() * (np.array(current_position[3:]) - np.array(current_position[1:]))\n","        social_component = c2 * self._get_random() * (np.array(global_best_position) - np.array(current_position[1:]))\n","        new_velocity = inertia + cognitive_component + social_component\n","        return new_velocity.tolist()\n","\n","    def _update_position(self, current_position, velocity):\n","        new_position = np.array(current_position) + np.array([0] + velocity)\n","        return new_position.tolist()\n","\n","    @staticmethod\n","    def _get_random():\n","        return np.random.uniform(low=0, high=1)\n","\n","    @staticmethod\n","    def _get_random_in_range(min_val, max_val):\n","        return np.random.uniform(low=min_val, high=max_val)\n","\n","    @staticmethod\n","    def _clip(value, min_val, max_val):\n","        return max(min(value, max_val), min_val)\n","# Creating a Particle Swarm Optimizer object\n","pso = ParticleSwarmOptimizer(num_particles, objective_function, param_boundaries)\n","# Finding the optimal parameters\n","optimal_params = pso.optimize(num_iterations)\n","# Displaying the best parameters found\n","print(f\"Best parameters found by PSO: {optimal_params}\")\n","# Compiling the model with the best parameters\n","model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=optimal_params[0]),\n","              loss='categorical_crossentropy',\n","              metrics=['accuracy'])\n","# Training the model with the best parameters\n","history = model.fit(X_train, y_train, batch_size=int(optimal_params[1]), epochs=int(optimal_params[2]), verbose=1)\n","# Evaluating the model on the test set\n","test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n","print(f\"Test loss: {test_loss}, Test accuracy: {test_accuracy}\")"],"metadata":{"id":"0c5UZue-nTaa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**5. Particle swarm  Optimization using SoftSign Activation function**"],"metadata":{"id":"ISXOK7DDnTPT"}},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras.utils import to_categorical\n","# Loading MNIST data set\n","mnist = tf.keras.datasets.mnist\n","(X_train, y_train), (X_test, y_test) = mnist.load_data()\n","# Normalizing the pixel values to the range [0, 1]\n","X_train, X_test = X_train / 255.0, X_test / 255.0\n","# Converting labels to categorical\n","y_train = to_categorical(y_train, 10)\n","y_test = to_categorical(y_test, 10)\n","# Converting labels to categorical\n","y_train = to_categorical(y_train, 10)\n","y_test = to_categorical(y_test, 10)\n","# CNN architecture\n","model = Sequential()\n","model.add(Conv2D(32, (3, 3), padding='same', activation='softsign', input_shape=(28, 28, 1)))\n","model.add(MaxPooling2D(pool_size=(2, 2)))\n","model.add(Flatten())\n","model.add(Dense(128, activation='softsign'))\n","model.add(Dense(10, activation='softmax'))\n","# Compiling the model\n","model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","# Define the objective function for Particle Swarm Optimization\n","def objective_function(params):\n","    learning_rate = params[0]\n","    batch_size = int(params[1])\n","    epochs = int(params[2])\n","\n","    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n","                  loss='categorical_crossentropy',\n","                  metrics=['accuracy'])\n","\n","    history = model.fit(X_train, y_train, validation_data=(X_test, y_test),\n","                        batch_size=batch_size, epochs=epochs, verbose=0)\n","\n","    train_loss = history.history['loss'][-1]\n","    train_accuracy = history.history['accuracy'][-1]\n","\n","    return train_loss + 1 - train_accuracy  # PSO minimizes the objective function, so maximize accuracy.\n","# Define the search space for PSO\n","param_boundaries = [\n","    (1e-6, 1e-4),  # Learning rate\n","    (32, 50),     # Batch size\n","    (5, 10)        # Epochs\n","]\n","num_particles= 50\n","# Number of iterations\n","num_iterations = 50\n","# Particle Swarm Optimization\n","class ParticleSwarmOptimizer:\n","    def __init__(self, num_particles, objective_function, param_boundaries):\n","        self.num_particles = num_particles\n","        self.objective_function = objective_function\n","        self.param_boundaries = param_boundaries\n","\n","    def optimize(self, num_iterations):\n","        # Initialization\n","        particles = [self._initialize_particle() for _ in range(self.num_particles)]\n","        global_best_position = particles[0][0]\n","        global_best_fitness = particles[0][-1]\n","\n","        for iteration in range(num_iterations):\n","            for i in range(self.num_particles):\n","                # Update velocity and position\n","                new_velocity = self._update_velocity(particles[i][1], particles[i][2], particles[i][3],\n","                                                     particles[i][0], global_best_position)\n","                new_position = self._update_position(particles[i][0], new_velocity)\n","\n","                # Ensure the search space boundaries\n","                new_position = [self._clip(val, *self.param_boundaries[idx]) for idx, val in enumerate(new_position)]\n","\n","                # Update fitness\n","                new_fitness = self.objective_function(new_position)\n","\n","                # Update personal best\n","                if new_fitness < particles[i][-1]:\n","                    particles[i] = [new_position, *new_velocity, new_fitness]\n","\n","                # Update global best\n","                if new_fitness < global_best_fitness:\n","                    global_best_position = new_position\n","                    global_best_fitness = new_fitness\n","\n","        # Returning the best solution found\n","        return global_best_position\n","\n","    def _initialize_particle(self):\n","        position = [self._get_random_in_range(*boundaries) for boundaries in self.param_boundaries]\n","        velocity = [self._get_random_in_range(-1, 1) for _ in range(len(self.param_boundaries))]\n","        fitness = self.objective_function(position)\n","        return [position, *velocity, fitness]\n","\n","    def _update_velocity(self, w, c1, c2, current_position, global_best_position):\n","        inertia = w * np.array(current_position[1:])\n","        cognitive_component = c1 * self._get_random() * (np.array(current_position[3:]) - np.array(current_position[1:]))\n","        social_component = c2 * self._get_random() * (np.array(global_best_position) - np.array(current_position[1:]))\n","        new_velocity = inertia + cognitive_component + social_component\n","        return new_velocity.tolist()\n","\n","    def _update_position(self, current_position, velocity):\n","        new_position = np.array(current_position) + np.array([0] + velocity)\n","        return new_position.tolist()\n","\n","    @staticmethod\n","    def _get_random():\n","        return np.random.uniform(low=0, high=1)\n","\n","    @staticmethod\n","    def _get_random_in_range(min_val, max_val):\n","        return np.random.uniform(low=min_val, high=max_val)\n","\n","    @staticmethod\n","    def _clip(value, min_val, max_val):\n","        return max(min(value, max_val), min_val)\n","# Creating a Particle Swarm Optimizer object\n","pso = ParticleSwarmOptimizer(num_particles, objective_function, param_boundaries)\n","# Finding the optimal parameters\n","optimal_params = pso.optimize(num_iterations)\n","# Displaying the best parameters found\n","print(f\"Best parameters found by PSO: {optimal_params}\")\n","# Compiling the model with the best parameters\n","model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=optimal_params[0]),\n","              loss='categorical_crossentropy',\n","              metrics=['accuracy'])\n","# Training the model with the best parameters\n","history = model.fit(X_train, y_train, batch_size=int(optimal_params[1]), epochs=int(optimal_params[2]), verbose=1)\n","# Evaluating the model on the test set\n","test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n","print(f\"Test loss: {test_loss}, Test accuracy: {test_accuracy}\")"],"metadata":{"id":"qgVEiSCcnTDN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**6. Particle swarm Optimization using SoftPlus Activation function**"],"metadata":{"id":"_p6pfu5lnS5R"}},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras.utils import to_categorical\n","# Loading MNIST data set\n","mnist = tf.keras.datasets.mnist\n","(X_train, y_train), (X_test, y_test) = mnist.load_data()\n","# Normalizing the pixel values to the range [0, 1]\n","X_train, X_test = X_train / 255.0, X_test / 255.0\n","# Converting labels to categorical\n","y_train = to_categorical(y_train, 10)\n","y_test = to_categorical(y_test, 10)\n","# Converting labels to categorical\n","y_train = to_categorical(y_train, 10)\n","y_test = to_categorical(y_test, 10)\n","# CNN architecture\n","model = Sequential()\n","model.add(Conv2D(32, (3, 3), padding='same', activation='softplus', input_shape=(28, 28, 1)))\n","model.add(MaxPooling2D(pool_size=(2, 2)))\n","model.add(Flatten())\n","model.add(Dense(128, activation='softplus'))\n","model.add(Dense(10, activation='softmax'))\n","# Compiling the model\n","model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","# Define the objective function for Particle Swarm Optimization\n","def objective_function(params):\n","    learning_rate = params[0]\n","    batch_size = int(params[1])\n","    epochs = int(params[2])\n","\n","    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n","                  loss='categorical_crossentropy',\n","                  metrics=['accuracy'])\n","\n","    history = model.fit(X_train, y_train, validation_data=(X_test, y_test),\n","                        batch_size=batch_size, epochs=epochs, verbose=0)\n","\n","    train_loss = history.history['loss'][-1]\n","    train_accuracy = history.history['accuracy'][-1]\n","\n","    return train_loss + 1 - train_accuracy  # PSO minimizes the objective function, so maximize accuracy.\n","# Define the search space for PSO\n","param_boundaries = [\n","    (1e-6, 1e-4),  # Learning rate\n","    (32, 50),     # Batch size\n","    (5, 10)        # Epochs\n","]\n","num_particles= 50\n","# Number of iterations\n","num_iterations = 50\n","# Particle Swarm Optimization\n","class ParticleSwarmOptimizer:\n","    def __init__(self, num_particles, objective_function, param_boundaries):\n","        self.num_particles = num_particles\n","        self.objective_function = objective_function\n","        self.param_boundaries = param_boundaries\n","\n","    def optimize(self, num_iterations):\n","        # Initialization\n","        particles = [self._initialize_particle() for _ in range(self.num_particles)]\n","        global_best_position = particles[0][0]\n","        global_best_fitness = particles[0][-1]\n","\n","        for iteration in range(num_iterations):\n","            for i in range(self.num_particles):\n","                # Update velocity and position\n","                new_velocity = self._update_velocity(particles[i][1], particles[i][2], particles[i][3],\n","                                                     particles[i][0], global_best_position)\n","                new_position = self._update_position(particles[i][0], new_velocity)\n","\n","                # Ensure the search space boundaries\n","                new_position = [self._clip(val, *self.param_boundaries[idx]) for idx, val in enumerate(new_position)]\n","\n","                # Update fitness\n","                new_fitness = self.objective_function(new_position)\n","\n","                # Update personal best\n","                if new_fitness < particles[i][-1]:\n","                    particles[i] = [new_position, *new_velocity, new_fitness]\n","\n","                # Update global best\n","                if new_fitness < global_best_fitness:\n","                    global_best_position = new_position\n","                    global_best_fitness = new_fitness\n","\n","        # Returning the best solution found\n","        return global_best_position\n","\n","    def _initialize_particle(self):\n","        position = [self._get_random_in_range(*boundaries) for boundaries in self.param_boundaries]\n","        velocity = [self._get_random_in_range(-1, 1) for _ in range(len(self.param_boundaries))]\n","        fitness = self.objective_function(position)\n","        return [position, *velocity, fitness]\n","\n","    def _update_velocity(self, w, c1, c2, current_position, global_best_position):\n","        inertia = w * np.array(current_position[1:])\n","        cognitive_component = c1 * self._get_random() * (np.array(current_position[3:]) - np.array(current_position[1:]))\n","        social_component = c2 * self._get_random() * (np.array(global_best_position) - np.array(current_position[1:]))\n","        new_velocity = inertia + cognitive_component + social_component\n","        return new_velocity.tolist()\n","\n","    def _update_position(self, current_position, velocity):\n","        new_position = np.array(current_position) + np.array([0] + velocity)\n","        return new_position.tolist()\n","\n","    @staticmethod\n","    def _get_random():\n","        return np.random.uniform(low=0, high=1)\n","\n","    @staticmethod\n","    def _get_random_in_range(min_val, max_val):\n","        return np.random.uniform(low=min_val, high=max_val)\n","\n","    @staticmethod\n","    def _clip(value, min_val, max_val):\n","        return max(min(value, max_val), min_val)\n","# Creating a Particle Swarm Optimizer object\n","pso = ParticleSwarmOptimizer(num_particles, objective_function, param_boundaries)\n","# Finding the optimal parameters\n","optimal_params = pso.optimize(num_iterations)\n","# Displaying the best parameters found\n","print(f\"Best parameters found by PSO: {optimal_params}\")\n","# Compiling the model with the best parameters\n","model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=optimal_params[0]),\n","              loss='categorical_crossentropy',\n","              metrics=['accuracy'])\n","# Training the model with the best parameters\n","history = model.fit(X_train, y_train, batch_size=int(optimal_params[1]), epochs=int(optimal_params[2]), verbose=1)\n","# Evaluating the model on the test set\n","test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n","print(f\"Test loss: {test_loss}, Test accuracy: {test_accuracy}\")"],"metadata":{"id":"1OwEChO-nStE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**7. Particle swarm  Optimization using swish Activation function**"],"metadata":{"id":"IjLgt7lvnScX"}},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras.utils import to_categorical\n","# Loading MNIST data set\n","mnist = tf.keras.datasets.mnist\n","(X_train, y_train), (X_test, y_test) = mnist.load_data()\n","# Normalizing the pixel values to the range [0, 1]\n","X_train, X_test = X_train / 255.0, X_test / 255.0\n","# Converting labels to categorical\n","y_train = to_categorical(y_train, 10)\n","y_test = to_categorical(y_test, 10)\n","# Converting labels to categorical\n","y_train = to_categorical(y_train, 10)\n","y_test = to_categorical(y_test, 10)\n","# CNN architecture\n","model = Sequential()\n","model.add(Conv2D(32, (3, 3), padding='same', activation='swish', input_shape=(28, 28, 1)))\n","model.add(MaxPooling2D(pool_size=(2, 2)))\n","model.add(Flatten())\n","model.add(Dense(128, activation='swish'))\n","model.add(Dense(10, activation='softmax'))\n","# Compiling the model\n","model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","# Define the objective function for Particle Swarm Optimization\n","def objective_function(params):\n","    learning_rate = params[0]\n","    batch_size = int(params[1])\n","    epochs = int(params[2])\n","\n","    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n","                  loss='categorical_crossentropy',\n","                  metrics=['accuracy'])\n","\n","    history = model.fit(X_train, y_train, validation_data=(X_test, y_test),\n","                        batch_size=batch_size, epochs=epochs, verbose=0)\n","\n","    train_loss = history.history['loss'][-1]\n","    train_accuracy = history.history['accuracy'][-1]\n","\n","    return train_loss + 1 - train_accuracy  # PSO minimizes the objective function, so maximize accuracy.\n","# Define the search space for PSO\n","param_boundaries = [\n","    (1e-6, 1e-4),  # Learning rate\n","    (32, 50),     # Batch size\n","    (5, 10)        # Epochs\n","]\n","num_particles= 50\n","# Number of iterations\n","num_iterations = 50\n","# Particle Swarm Optimization\n","class ParticleSwarmOptimizer:\n","    def __init__(self, num_particles, objective_function, param_boundaries):\n","        self.num_particles = num_particles\n","        self.objective_function = objective_function\n","        self.param_boundaries = param_boundaries\n","\n","    def optimize(self, num_iterations):\n","        # Initialization\n","        particles = [self._initialize_particle() for _ in range(self.num_particles)]\n","        global_best_position = particles[0][0]\n","        global_best_fitness = particles[0][-1]\n","\n","        for iteration in range(num_iterations):\n","            for i in range(self.num_particles):\n","                # Update velocity and position\n","                new_velocity = self._update_velocity(particles[i][1], particles[i][2], particles[i][3],\n","                                                     particles[i][0], global_best_position)\n","                new_position = self._update_position(particles[i][0], new_velocity)\n","\n","                # Ensure the search space boundaries\n","                new_position = [self._clip(val, *self.param_boundaries[idx]) for idx, val in enumerate(new_position)]\n","\n","                # Update fitness\n","                new_fitness = self.objective_function(new_position)\n","\n","                # Update personal best\n","                if new_fitness < particles[i][-1]:\n","                    particles[i] = [new_position, *new_velocity, new_fitness]\n","\n","                # Update global best\n","                if new_fitness < global_best_fitness:\n","                    global_best_position = new_position\n","                    global_best_fitness = new_fitness\n","\n","        # Returning the best solution found\n","        return global_best_position\n","\n","    def _initialize_particle(self):\n","        position = [self._get_random_in_range(*boundaries) for boundaries in self.param_boundaries]\n","        velocity = [self._get_random_in_range(-1, 1) for _ in range(len(self.param_boundaries))]\n","        fitness = self.objective_function(position)\n","        return [position, *velocity, fitness]\n","\n","    def _update_velocity(self, w, c1, c2, current_position, global_best_position):\n","        inertia = w * np.array(current_position[1:])\n","        cognitive_component = c1 * self._get_random() * (np.array(current_position[3:]) - np.array(current_position[1:]))\n","        social_component = c2 * self._get_random() * (np.array(global_best_position) - np.array(current_position[1:]))\n","        new_velocity = inertia + cognitive_component + social_component\n","        return new_velocity.tolist()\n","\n","    def _update_position(self, current_position, velocity):\n","        new_position = np.array(current_position) + np.array([0] + velocity)\n","        return new_position.tolist()\n","\n","    @staticmethod\n","    def _get_random():\n","        return np.random.uniform(low=0, high=1)\n","\n","    @staticmethod\n","    def _get_random_in_range(min_val, max_val):\n","        return np.random.uniform(low=min_val, high=max_val)\n","\n","    @staticmethod\n","    def _clip(value, min_val, max_val):\n","        return max(min(value, max_val), min_val)\n","# Creating a Particle Swarm Optimizer object\n","pso = ParticleSwarmOptimizer(num_particles, objective_function, param_boundaries)\n","# Finding the optimal parameters\n","optimal_params = pso.optimize(num_iterations)\n","# Displaying the best parameters found\n","print(f\"Best parameters found by PSO: {optimal_params}\")\n","# Compiling the model with the best parameters\n","model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=optimal_params[0]),\n","              loss='categorical_crossentropy',\n","              metrics=['accuracy'])\n","# Training the model with the best parameters\n","history = model.fit(X_train, y_train, batch_size=int(optimal_params[1]), epochs=int(optimal_params[2]), verbose=1)\n","# Evaluating the model on the test set\n","test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n","print(f\"Test loss: {test_loss}, Test accuracy: {test_accuracy}\")"],"metadata":{"id":"CSzxlVNKnSIA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**8. Particle swarm Optimization using mish Activation function**"],"metadata":{"id":"BxnvzfQBnRuw"}},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras.utils import to_categorical\n","# Loading MNIST data set\n","mnist = tf.keras.datasets.mnist\n","(X_train, y_train), (X_test, y_test) = mnist.load_data()\n","# Normalizing the pixel values to the range [0, 1]\n","X_train, X_test = X_train / 255.0, X_test / 255.0\n","# Converting labels to categorical\n","y_train = to_categorical(y_train, 10)\n","y_test = to_categorical(y_test, 10)\n","# Converting labels to categorical\n","y_train = to_categorical(y_train, 10)\n","y_test = to_categorical(y_test, 10)\n","# CNN architecture\n","model = Sequential()\n","model.add(Conv2D(32, (3, 3), padding='same', activation='mish', input_shape=(28, 28, 1)))\n","model.add(MaxPooling2D(pool_size=(2, 2)))\n","model.add(Flatten())\n","model.add(Dense(128, activation='mish'))\n","model.add(Dense(10, activation='softmax'))\n","# Compiling the model\n","model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","# Define the objective function for Particle Swarm Optimization\n","def objective_function(params):\n","    learning_rate = params[0]\n","    batch_size = int(params[1])\n","    epochs = int(params[2])\n","\n","    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n","                  loss='categorical_crossentropy',\n","                  metrics=['accuracy'])\n","\n","    history = model.fit(X_train, y_train, validation_data=(X_test, y_test),\n","                        batch_size=batch_size, epochs=epochs, verbose=0)\n","\n","    train_loss = history.history['loss'][-1]\n","    train_accuracy = history.history['accuracy'][-1]\n","\n","    return train_loss + 1 - train_accuracy  # PSO minimizes the objective function, so maximize accuracy.\n","# Define the search space for PSO\n","param_boundaries = [\n","    (1e-6, 1e-4),  # Learning rate\n","    (32, 50),     # Batch size\n","    (5, 10)        # Epochs\n","]\n","num_particles= 50\n","# Number of iterations\n","num_iterations = 50\n","# Particle Swarm Optimization\n","class ParticleSwarmOptimizer:\n","    def __init__(self, num_particles, objective_function, param_boundaries):\n","        self.num_particles = num_particles\n","        self.objective_function = objective_function\n","        self.param_boundaries = param_boundaries\n","\n","    def optimize(self, num_iterations):\n","        # Initialization\n","        particles = [self._initialize_particle() for _ in range(self.num_particles)]\n","        global_best_position = particles[0][0]\n","        global_best_fitness = particles[0][-1]\n","\n","        for iteration in range(num_iterations):\n","            for i in range(self.num_particles):\n","                # Update velocity and position\n","                new_velocity = self._update_velocity(particles[i][1], particles[i][2], particles[i][3],\n","                                                     particles[i][0], global_best_position)\n","                new_position = self._update_position(particles[i][0], new_velocity)\n","\n","                # Ensure the search space boundaries\n","                new_position = [self._clip(val, *self.param_boundaries[idx]) for idx, val in enumerate(new_position)]\n","\n","                # Update fitness\n","                new_fitness = self.objective_function(new_position)\n","\n","                # Update personal best\n","                if new_fitness < particles[i][-1]:\n","                    particles[i] = [new_position, *new_velocity, new_fitness]\n","\n","                # Update global best\n","                if new_fitness < global_best_fitness:\n","                    global_best_position = new_position\n","                    global_best_fitness = new_fitness\n","\n","        # Returning the best solution found\n","        return global_best_position\n","\n","    def _initialize_particle(self):\n","        position = [self._get_random_in_range(*boundaries) for boundaries in self.param_boundaries]\n","        velocity = [self._get_random_in_range(-1, 1) for _ in range(len(self.param_boundaries))]\n","        fitness = self.objective_function(position)\n","        return [position, *velocity, fitness]\n","\n","    def _update_velocity(self, w, c1, c2, current_position, global_best_position):\n","        inertia = w * np.array(current_position[1:])\n","        cognitive_component = c1 * self._get_random() * (np.array(current_position[3:]) - np.array(current_position[1:]))\n","        social_component = c2 * self._get_random() * (np.array(global_best_position) - np.array(current_position[1:]))\n","        new_velocity = inertia + cognitive_component + social_component\n","        return new_velocity.tolist()\n","\n","    def _update_position(self, current_position, velocity):\n","        new_position = np.array(current_position) + np.array([0] + velocity)\n","        return new_position.tolist()\n","\n","    @staticmethod\n","    def _get_random():\n","        return np.random.uniform(low=0, high=1)\n","\n","    @staticmethod\n","    def _get_random_in_range(min_val, max_val):\n","        return np.random.uniform(low=min_val, high=max_val)\n","\n","    @staticmethod\n","    def _clip(value, min_val, max_val):\n","        return max(min(value, max_val), min_val)\n","# Creating a Particle Swarm Optimizer object\n","pso = ParticleSwarmOptimizer(num_particles, objective_function, param_boundaries)\n","# Finding the optimal parameters\n","optimal_params = pso.optimize(num_iterations)\n","# Displaying the best parameters found\n","print(f\"Best parameters found by PSO: {optimal_params}\")\n","# Compiling the model with the best parameters\n","model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=optimal_params[0]),\n","              loss='categorical_crossentropy',\n","              metrics=['accuracy'])\n","# Training the model with the best parameters\n","history = model.fit(X_train, y_train, batch_size=int(optimal_params[1]), epochs=int(optimal_params[2]), verbose=1)\n","# Evaluating the model on the test set\n","test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n","print(f\"Test loss: {test_loss}, Test accuracy: {test_accuracy}\")"],"metadata":{"id":"ur8XqBHFoCB5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**9. Particle swarm Optimization using HardSigmoid Activation function**"],"metadata":{"id":"INVfg0O9nRBK"}},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras.utils import to_categorical\n","# Loading MNIST data set\n","mnist = tf.keras.datasets.mnist\n","(X_train, y_train), (X_test, y_test) = mnist.load_data()\n","# Normalizing the pixel values to the range [0, 1]\n","X_train, X_test = X_train / 255.0, X_test / 255.0\n","# Converting labels to categorical\n","y_train = to_categorical(y_train, 10)\n","y_test = to_categorical(y_test, 10)\n","# Converting labels to categorical\n","y_train = to_categorical(y_train, 10)\n","y_test = to_categorical(y_test, 10)\n","# Define the Hard Sigmoid Activation Layer\n","class HardSigmoidActivation(tf.keras.layers.Layer):\n","    def __init__(self, **kwargs):\n","        super(HardSigmoidActivation, self).__init__(**kwargs)\n","\n","    def call(self, inputs):\n","        return tf.keras.backend.hard_sigmoid(inputs)\n","\n","def create_cnn_model(activation_function):\n","    model = Sequential()\n","    model.add(Conv2D(32, (3, 3), padding='same', activation=activation_function, input_shape=(28, 28, 1)))\n","    model.add(MaxPooling2D(pool_size=(2, 2)))\n","    model.add(Flatten())\n","    model.add(Dense(128, activation=activation_function))\n","    model.add(Dense(10, activation='softmax'))\n","    return model\n","# Compiling the model\n","    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","# Define the objective function for Particle Swarm Optimization\n","def objective_function(params):\n","    learning_rate = params[0]\n","    batch_size = int(params[1])\n","    epochs = int(params[2])\n","\n","    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n","                  loss='categorical_crossentropy',\n","                  metrics=['accuracy'])\n","\n","    history = model.fit(X_train, y_train, validation_data=(X_test, y_test),\n","                        batch_size=batch_size, epochs=epochs, verbose=0)\n","\n","    train_loss = history.history['loss'][-1]\n","    train_accuracy = history.history['accuracy'][-1]\n","\n","    return train_loss + 1 - train_accuracy  # PSO minimizes the objective function, so maximize accuracy.\n","# Define the search space for PSO\n","param_boundaries = [\n","    (1e-6, 1e-4),  # Learning rate\n","    (32, 50),     # Batch size\n","    (5, 10)        # Epochs\n","]\n","num_particles= 50\n","# Number of iterations\n","num_iterations = 50\n","# Particle Swarm Optimization\n","class ParticleSwarmOptimizer:\n","    def __init__(self, num_particles, objective_function, param_boundaries):\n","        self.num_particles = num_particles\n","        self.objective_function = objective_function\n","        self.param_boundaries = param_boundaries\n","\n","    def optimize(self, num_iterations):\n","        # Initialization\n","        particles = [self._initialize_particle() for _ in range(self.num_particles)]\n","        global_best_position = particles[0][0]\n","        global_best_fitness = particles[0][-1]\n","\n","        for iteration in range(num_iterations):\n","            for i in range(self.num_particles):\n","                # Update velocity and position\n","                new_velocity = self._update_velocity(particles[i][1], particles[i][2], particles[i][3],\n","                                                     particles[i][0], global_best_position)\n","                new_position = self._update_position(particles[i][0], new_velocity)\n","\n","                # Ensure the search space boundaries\n","                new_position = [self._clip(val, *self.param_boundaries[idx]) for idx, val in enumerate(new_position)]\n","\n","                # Update fitness\n","                new_fitness = self.objective_function(new_position)\n","\n","                # Update personal best\n","                if new_fitness < particles[i][-1]:\n","                    particles[i] = [new_position, *new_velocity, new_fitness]\n","\n","                # Update global best\n","                if new_fitness < global_best_fitness:\n","                    global_best_position = new_position\n","                    global_best_fitness = new_fitness\n","\n","        # Returning the best solution found\n","        return global_best_position\n","\n","    def _initialize_particle(self):\n","        position = [self._get_random_in_range(*boundaries) for boundaries in self.param_boundaries]\n","        velocity = [self._get_random_in_range(-1, 1) for _ in range(len(self.param_boundaries))]\n","        fitness = self.objective_function(position)\n","        return [position, *velocity, fitness]\n","\n","    def _update_velocity(self, w, c1, c2, current_position, global_best_position):\n","        inertia = w * np.array(current_position[1:])\n","        cognitive_component = c1 * self._get_random() * (np.array(current_position[3:]) - np.array(current_position[1:]))\n","        social_component = c2 * self._get_random() * (np.array(global_best_position) - np.array(current_position[1:]))\n","        new_velocity = inertia + cognitive_component + social_component\n","        return new_velocity.tolist()\n","\n","    def _update_position(self, current_position, velocity):\n","        new_position = np.array(current_position) + np.array([0] + velocity)\n","        return new_position.tolist()\n","\n","    @staticmethod\n","    def _get_random():\n","        return np.random.uniform(low=0, high=1)\n","\n","    @staticmethod\n","    def _get_random_in_range(min_val, max_val):\n","        return np.random.uniform(low=min_val, high=max_val)\n","\n","    @staticmethod\n","    def _clip(value, min_val, max_val):\n","        return max(min(value, max_val), min_val)\n","# Creating a Particle Swarm Optimizer object\n","pso = ParticleSwarmOptimizer(num_particles, objective_function, param_boundaries)\n","# Finding the optimal parameters\n","optimal_params = pso.optimize(num_iterations)\n","# Displaying the best parameters found\n","print(f\"Best parameters found by PSO: {optimal_params}\")\n","# Compiling the model with the best parameters\n","model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=optimal_params[0]),\n","              loss='categorical_crossentropy',\n","              metrics=['accuracy'])\n","# Training the model with the best parameters\n","history = model.fit(X_train, y_train, batch_size=int(optimal_params[1]), epochs=int(optimal_params[2]), verbose=1)\n","# Evaluating the model on the test set\n","test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n","print(f\"Test loss: {test_loss}, Test accuracy: {test_accuracy}\")"],"metadata":{"id":"M3i9L8pIoKO6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**10.Particle swarm Optimization using selu Activation function**"],"metadata":{"id":"B0V9wMz7oKh-"}},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras.utils import to_categorical\n","# Loading MNIST data set\n","mnist = tf.keras.datasets.mnist\n","(X_train, y_train), (X_test, y_test) = mnist.load_data()\n","# Normalizing the pixel values to the range [0, 1]\n","X_train, X_test = X_train / 255.0, X_test / 255.0\n","# Converting labels to categorical\n","y_train = to_categorical(y_train, 10)\n","y_test = to_categorical(y_test, 10)\n","# Converting labels to categorical\n","y_train = to_categorical(y_train, 10)\n","y_test = to_categorical(y_test, 10)\n","# CNN architecture\n","model = Sequential()\n","model.add(Conv2D(32, (3, 3), padding='same', activation='selu', input_shape=(28, 28, 1)))\n","model.add(MaxPooling2D(pool_size=(2, 2)))\n","model.add(Flatten())\n","model.add(Dense(128, activation='selu'))\n","model.add(Dense(10, activation='softmax'))\n","# Compiling the model\n","model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","# Define the objective function for Particle Swarm Optimization\n","def objective_function(params):\n","    learning_rate = params[0]\n","    batch_size = int(params[1])\n","    epochs = int(params[2])\n","\n","    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n","                  loss='categorical_crossentropy',\n","                  metrics=['accuracy'])\n","\n","    history = model.fit(X_train, y_train, validation_data=(X_test, y_test),\n","                        batch_size=batch_size, epochs=epochs, verbose=0)\n","\n","    train_loss = history.history['loss'][-1]\n","    train_accuracy = history.history['accuracy'][-1]\n","\n","    return train_loss + 1 - train_accuracy  # PSO minimizes the objective function, so maximize accuracy.\n","# Define the search space for PSO\n","param_boundaries = [\n","    (1e-6, 1e-4),  # Learning rate\n","    (32, 50),     # Batch size\n","    (5, 10)        # Epochs\n","]\n","num_particles= 50\n","# Number of iterations\n","num_iterations = 50\n","# Particle Swarm Optimization\n","class ParticleSwarmOptimizer:\n","    def __init__(self, num_particles, objective_function, param_boundaries):\n","        self.num_particles = num_particles\n","        self.objective_function = objective_function\n","        self.param_boundaries = param_boundaries\n","\n","    def optimize(self, num_iterations):\n","        # Initialization\n","        particles = [self._initialize_particle() for _ in range(self.num_particles)]\n","        global_best_position = particles[0][0]\n","        global_best_fitness = particles[0][-1]\n","\n","        for iteration in range(num_iterations):\n","            for i in range(self.num_particles):\n","                # Update velocity and position\n","                new_velocity = self._update_velocity(particles[i][1], particles[i][2], particles[i][3],\n","                                                     particles[i][0], global_best_position)\n","                new_position = self._update_position(particles[i][0], new_velocity)\n","\n","                # Ensure the search space boundaries\n","                new_position = [self._clip(val, *self.param_boundaries[idx]) for idx, val in enumerate(new_position)]\n","\n","                # Update fitness\n","                new_fitness = self.objective_function(new_position)\n","\n","                # Update personal best\n","                if new_fitness < particles[i][-1]:\n","                    particles[i] = [new_position, *new_velocity, new_fitness]\n","\n","                # Update global best\n","                if new_fitness < global_best_fitness:\n","                    global_best_position = new_position\n","                    global_best_fitness = new_fitness\n","\n","        # Returning the best solution found\n","        return global_best_position\n","\n","    def _initialize_particle(self):\n","        position = [self._get_random_in_range(*boundaries) for boundaries in self.param_boundaries]\n","        velocity = [self._get_random_in_range(-1, 1) for _ in range(len(self.param_boundaries))]\n","        fitness = self.objective_function(position)\n","        return [position, *velocity, fitness]\n","\n","    def _update_velocity(self, w, c1, c2, current_position, global_best_position):\n","        inertia = w * np.array(current_position[1:])\n","        cognitive_component = c1 * self._get_random() * (np.array(current_position[3:]) - np.array(current_position[1:]))\n","        social_component = c2 * self._get_random() * (np.array(global_best_position) - np.array(current_position[1:]))\n","        new_velocity = inertia + cognitive_component + social_component\n","        return new_velocity.tolist()\n","\n","    def _update_position(self, current_position, velocity):\n","        new_position = np.array(current_position) + np.array([0] + velocity)\n","        return new_position.tolist()\n","\n","    @staticmethod\n","    def _get_random():\n","        return np.random.uniform(low=0, high=1)\n","\n","    @staticmethod\n","    def _get_random_in_range(min_val, max_val):\n","        return np.random.uniform(low=min_val, high=max_val)\n","\n","    @staticmethod\n","    def _clip(value, min_val, max_val):\n","        return max(min(value, max_val), min_val)\n","# Creating a Particle Swarm Optimizer object\n","pso = ParticleSwarmOptimizer(num_particles, objective_function, param_boundaries)\n","# Finding the optimal parameters\n","optimal_params = pso.optimize(num_iterations)\n","# Displaying the best parameters found\n","print(f\"Best parameters found by PSO: {optimal_params}\")\n","# Compiling the model with the best parameters\n","model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=optimal_params[0]),\n","              loss='categorical_crossentropy',\n","              metrics=['accuracy'])\n","# Training the model with the best parameters\n","history = model.fit(X_train, y_train, batch_size=int(optimal_params[1]), epochs=int(optimal_params[2]), verbose=1)\n","# Evaluating the model on the test set\n","test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n","print(f\"Test loss: {test_loss}, Test accuracy: {test_accuracy}\")"],"metadata":{"id":"QuAsR1T0oMHH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**11. Particle swarm  Optimization using elu Activation function**"],"metadata":{"id":"JJRE0UvmoMWW"}},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras.utils import to_categorical\n","# Loading MNIST data set\n","mnist = tf.keras.datasets.mnist\n","(X_train, y_train), (X_test, y_test) = mnist.load_data()\n","# Normalizing the pixel values to the range [0, 1]\n","X_train, X_test = X_train / 255.0, X_test / 255.0\n","# Converting labels to categorical\n","y_train = to_categorical(y_train, 10)\n","y_test = to_categorical(y_test, 10)\n","# Converting labels to categorical\n","y_train = to_categorical(y_train, 10)\n","y_test = to_categorical(y_test, 10)\n","# CNN architecture\n","model = Sequential()\n","model.add(Conv2D(32, (3, 3), padding='same', activation='elu', input_shape=(28, 28, 1)))\n","model.add(MaxPooling2D(pool_size=(2, 2)))\n","model.add(Flatten())\n","model.add(Dense(128, activation='elu'))\n","model.add(Dense(10, activation='softmax'))\n","# Compiling the model\n","model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","# Define the objective function for Particle Swarm Optimization\n","def objective_function(params):\n","    learning_rate = params[0]\n","    batch_size = int(params[1])\n","    epochs = int(params[2])\n","\n","    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n","                  loss='categorical_crossentropy',\n","                  metrics=['accuracy'])\n","\n","    history = model.fit(X_train, y_train, validation_data=(X_test, y_test),\n","                        batch_size=batch_size, epochs=epochs, verbose=0)\n","\n","    train_loss = history.history['loss'][-1]\n","    train_accuracy = history.history['accuracy'][-1]\n","\n","    return train_loss + 1 - train_accuracy  # PSO minimizes the objective function, so maximize accuracy.\n","# Define the search space for PSO\n","param_boundaries = [\n","    (1e-6, 1e-4),  # Learning rate\n","    (32, 50),     # Batch size\n","    (5, 10)        # Epochs\n","]\n","num_particles= 50\n","# Number of iterations\n","num_iterations = 50\n","# Particle Swarm Optimization\n","class ParticleSwarmOptimizer:\n","    def __init__(self, num_particles, objective_function, param_boundaries):\n","        self.num_particles = num_particles\n","        self.objective_function = objective_function\n","        self.param_boundaries = param_boundaries\n","\n","    def optimize(self, num_iterations):\n","        # Initialization\n","        particles = [self._initialize_particle() for _ in range(self.num_particles)]\n","        global_best_position = particles[0][0]\n","        global_best_fitness = particles[0][-1]\n","\n","        for iteration in range(num_iterations):\n","            for i in range(self.num_particles):\n","                # Update velocity and position\n","                new_velocity = self._update_velocity(particles[i][1], particles[i][2], particles[i][3],\n","                                                     particles[i][0], global_best_position)\n","                new_position = self._update_position(particles[i][0], new_velocity)\n","\n","                # Ensure the search space boundaries\n","                new_position = [self._clip(val, *self.param_boundaries[idx]) for idx, val in enumerate(new_position)]\n","\n","                # Update fitness\n","                new_fitness = self.objective_function(new_position)\n","\n","                # Update personal best\n","                if new_fitness < particles[i][-1]:\n","                    particles[i] = [new_position, *new_velocity, new_fitness]\n","\n","                # Update global best\n","                if new_fitness < global_best_fitness:\n","                    global_best_position = new_position\n","                    global_best_fitness = new_fitness\n","\n","        # Returning the best solution found\n","        return global_best_position\n","\n","    def _initialize_particle(self):\n","        position = [self._get_random_in_range(*boundaries) for boundaries in self.param_boundaries]\n","        velocity = [self._get_random_in_range(-1, 1) for _ in range(len(self.param_boundaries))]\n","        fitness = self.objective_function(position)\n","        return [position, *velocity, fitness]\n","\n","    def _update_velocity(self, w, c1, c2, current_position, global_best_position):\n","        inertia = w * np.array(current_position[1:])\n","        cognitive_component = c1 * self._get_random() * (np.array(current_position[3:]) - np.array(current_position[1:]))\n","        social_component = c2 * self._get_random() * (np.array(global_best_position) - np.array(current_position[1:]))\n","        new_velocity = inertia + cognitive_component + social_component\n","        return new_velocity.tolist()\n","\n","    def _update_position(self, current_position, velocity):\n","        new_position = np.array(current_position) + np.array([0] + velocity)\n","        return new_position.tolist()\n","\n","    @staticmethod\n","    def _get_random():\n","        return np.random.uniform(low=0, high=1)\n","\n","    @staticmethod\n","    def _get_random_in_range(min_val, max_val):\n","        return np.random.uniform(low=min_val, high=max_val)\n","\n","    @staticmethod\n","    def _clip(value, min_val, max_val):\n","        return max(min(value, max_val), min_val)\n","# Creating a Particle Swarm Optimizer object\n","pso = ParticleSwarmOptimizer(num_particles, objective_function, param_boundaries)\n","# Finding the optimal parameters\n","optimal_params = pso.optimize(num_iterations)\n","# Displaying the best parameters found\n","print(f\"Best parameters found by PSO: {optimal_params}\")\n","# Compiling the model with the best parameters\n","model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=optimal_params[0]),\n","              loss='categorical_crossentropy',\n","              metrics=['accuracy'])\n","# Training the model with the best parameters\n","history = model.fit(X_train, y_train, batch_size=int(optimal_params[1]), epochs=int(optimal_params[2]), verbose=1)\n","# Evaluating the model on the test set\n","test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n","print(f\"Test loss: {test_loss}, Test accuracy: {test_accuracy}\")"],"metadata":{"id":"8ADRj27LoQTY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**12. Particle swarm  Optimization using PRelu Activation function**"],"metadata":{"id":"UJQIkPHSoQw3"}},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras.utils import to_categorical\n","# Loading MNIST data set\n","mnist = tf.keras.datasets.mnist\n","(X_train, y_train), (X_test, y_test) = mnist.load_data()\n","# Normalizing the pixel values to the range [0, 1]\n","X_train, X_test = X_train / 255.0, X_test / 255.0\n","# Converting labels to categorical\n","y_train = to_categorical(y_train, 10)\n","y_test = to_categorical(y_test, 10)\n","# Converting labels to categorical\n","y_train = to_categorical(y_train, 10)\n","y_test = to_categorical(y_test, 10)\n","# CNN architecture\n","model = Sequential()\n","model.add(Conv2D(32, (3, 3), padding='same', activation='PRelu', input_shape=(28, 28, 1)))\n","model.add(MaxPooling2D(pool_size=(2, 2)))\n","model.add(Flatten())\n","model.add(Dense(128, activation='PRelu'))\n","model.add(Dense(10, activation='softmax'))\n","# Compiling the model\n","model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","# Define the objective function for Particle Swarm Optimization\n","def objective_function(params):\n","    learning_rate = params[0]\n","    batch_size = int(params[1])\n","    epochs = int(params[2])\n","\n","    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n","                  loss='categorical_crossentropy',\n","                  metrics=['accuracy'])\n","\n","    history = model.fit(X_train, y_train, validation_data=(X_test, y_test),\n","                        batch_size=batch_size, epochs=epochs, verbose=0)\n","\n","    train_loss = history.history['loss'][-1]\n","    train_accuracy = history.history['accuracy'][-1]\n","\n","    return train_loss + 1 - train_accuracy  # PSO minimizes the objective function, so maximize accuracy.\n","# Define the search space for PSO\n","param_boundaries = [\n","    (1e-6, 1e-4),  # Learning rate\n","    (32, 50),     # Batch size\n","    (5, 10)        # Epochs\n","]\n","num_particles= 50\n","# Number of iterations\n","num_iterations = 50\n","# Particle Swarm Optimization\n","class ParticleSwarmOptimizer:\n","    def __init__(self, num_particles, objective_function, param_boundaries):\n","        self.num_particles = num_particles\n","        self.objective_function = objective_function\n","        self.param_boundaries = param_boundaries\n","\n","    def optimize(self, num_iterations):\n","        # Initialization\n","        particles = [self._initialize_particle() for _ in range(self.num_particles)]\n","        global_best_position = particles[0][0]\n","        global_best_fitness = particles[0][-1]\n","\n","        for iteration in range(num_iterations):\n","            for i in range(self.num_particles):\n","                # Update velocity and position\n","                new_velocity = self._update_velocity(particles[i][1], particles[i][2], particles[i][3],\n","                                                     particles[i][0], global_best_position)\n","                new_position = self._update_position(particles[i][0], new_velocity)\n","\n","                # Ensure the search space boundaries\n","                new_position = [self._clip(val, *self.param_boundaries[idx]) for idx, val in enumerate(new_position)]\n","\n","                # Update fitness\n","                new_fitness = self.objective_function(new_position)\n","\n","                # Update personal best\n","                if new_fitness < particles[i][-1]:\n","                    particles[i] = [new_position, *new_velocity, new_fitness]\n","\n","                # Update global best\n","                if new_fitness < global_best_fitness:\n","                    global_best_position = new_position\n","                    global_best_fitness = new_fitness\n","\n","        # Returning the best solution found\n","        return global_best_position\n","\n","    def _initialize_particle(self):\n","        position = [self._get_random_in_range(*boundaries) for boundaries in self.param_boundaries]\n","        velocity = [self._get_random_in_range(-1, 1) for _ in range(len(self.param_boundaries))]\n","        fitness = self.objective_function(position)\n","        return [position, *velocity, fitness]\n","\n","    def _update_velocity(self, w, c1, c2, current_position, global_best_position):\n","        inertia = w * np.array(current_position[1:])\n","        cognitive_component = c1 * self._get_random() * (np.array(current_position[3:]) - np.array(current_position[1:]))\n","        social_component = c2 * self._get_random() * (np.array(global_best_position) - np.array(current_position[1:]))\n","        new_velocity = inertia + cognitive_component + social_component\n","        return new_velocity.tolist()\n","\n","    def _update_position(self, current_position, velocity):\n","        new_position = np.array(current_position) + np.array([0] + velocity)\n","        return new_position.tolist()\n","\n","    @staticmethod\n","    def _get_random():\n","        return np.random.uniform(low=0, high=1)\n","\n","    @staticmethod\n","    def _get_random_in_range(min_val, max_val):\n","        return np.random.uniform(low=min_val, high=max_val)\n","\n","    @staticmethod\n","    def _clip(value, min_val, max_val):\n","        return max(min(value, max_val), min_val)\n","# Creating a Particle Swarm Optimizer object\n","pso = ParticleSwarmOptimizer(num_particles, objective_function, param_boundaries)\n","# Finding the optimal parameters\n","optimal_params = pso.optimize(num_iterations)\n","# Displaying the best parameters found\n","print(f\"Best parameters found by PSO: {optimal_params}\")\n","# Compiling the model with the best parameters\n","model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=optimal_params[0]),\n","              loss='categorical_crossentropy',\n","              metrics=['accuracy'])\n","# Training the model with the best parameters\n","history = model.fit(X_train, y_train, batch_size=int(optimal_params[1]), epochs=int(optimal_params[2]), verbose=1)\n","# Evaluating the model on the test set\n","test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n","print(f\"Test loss: {test_loss}, Test accuracy: {test_accuracy}\")"],"metadata":{"id":"QzhkLQJVoUhv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**13. Particle swarm  Optimization using LeakyRelu Activation function**"],"metadata":{"id":"UE7hYwOloUwZ"}},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras.utils import to_categorical\n","# Loading MNIST data set\n","mnist = tf.keras.datasets.mnist\n","(X_train, y_train), (X_test, y_test) = mnist.load_data()\n","# Normalizing the pixel values to the range [0, 1]\n","X_train, X_test = X_train / 255.0, X_test / 255.0\n","# Converting labels to categorical\n","y_train = to_categorical(y_train, 10)\n","y_test = to_categorical(y_test, 10)\n","# Converting labels to categorical\n","y_train = to_categorical(y_train, 10)\n","y_test = to_categorical(y_test, 10)\n","# CNN architecture\n","model = Sequential()\n","model.add(Conv2D(32, (3, 3), padding='same', activation='LeakyRelu', input_shape=(28, 28, 1)))\n","model.add(MaxPooling2D(pool_size=(2, 2)))\n","model.add(Flatten())\n","model.add(Dense(128, activation='LeakyRelu'))\n","model.add(Dense(10, activation='softmax'))\n","# Compiling the model\n","model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","# Define the objective function for Particle Swarm Optimization\n","def objective_function(params):\n","    learning_rate = params[0]\n","    batch_size = int(params[1])\n","    epochs = int(params[2])\n","\n","    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n","                  loss='categorical_crossentropy',\n","                  metrics=['accuracy'])\n","\n","    history = model.fit(X_train, y_train, validation_data=(X_test, y_test),\n","                        batch_size=batch_size, epochs=epochs, verbose=0)\n","\n","    train_loss = history.history['loss'][-1]\n","    train_accuracy = history.history['accuracy'][-1]\n","\n","    return train_loss + 1 - train_accuracy  # PSO minimizes the objective function, so maximize accuracy.\n","# Define the search space for PSO\n","param_boundaries = [\n","    (1e-6, 1e-4),  # Learning rate\n","    (32, 50),     # Batch size\n","    (5, 10)        # Epochs\n","]\n","num_particles= 50\n","# Number of iterations\n","num_iterations = 50\n","# Particle Swarm Optimization\n","class ParticleSwarmOptimizer:\n","    def __init__(self, num_particles, objective_function, param_boundaries):\n","        self.num_particles = num_particles\n","        self.objective_function = objective_function\n","        self.param_boundaries = param_boundaries\n","\n","    def optimize(self, num_iterations):\n","        # Initialization\n","        particles = [self._initialize_particle() for _ in range(self.num_particles)]\n","        global_best_position = particles[0][0]\n","        global_best_fitness = particles[0][-1]\n","\n","        for iteration in range(num_iterations):\n","            for i in range(self.num_particles):\n","                # Update velocity and position\n","                new_velocity = self._update_velocity(particles[i][1], particles[i][2], particles[i][3],\n","                                                     particles[i][0], global_best_position)\n","                new_position = self._update_position(particles[i][0], new_velocity)\n","\n","                # Ensure the search space boundaries\n","                new_position = [self._clip(val, *self.param_boundaries[idx]) for idx, val in enumerate(new_position)]\n","\n","                # Update fitness\n","                new_fitness = self.objective_function(new_position)\n","\n","                # Update personal best\n","                if new_fitness < particles[i][-1]:\n","                    particles[i] = [new_position, *new_velocity, new_fitness]\n","\n","                # Update global best\n","                if new_fitness < global_best_fitness:\n","                    global_best_position = new_position\n","                    global_best_fitness = new_fitness\n","\n","        # Returning the best solution found\n","        return global_best_position\n","\n","    def _initialize_particle(self):\n","        position = [self._get_random_in_range(*boundaries) for boundaries in self.param_boundaries]\n","        velocity = [self._get_random_in_range(-1, 1) for _ in range(len(self.param_boundaries))]\n","        fitness = self.objective_function(position)\n","        return [position, *velocity, fitness]\n","\n","    def _update_velocity(self, w, c1, c2, current_position, global_best_position):\n","        inertia = w * np.array(current_position[1:])\n","        cognitive_component = c1 * self._get_random() * (np.array(current_position[3:]) - np.array(current_position[1:]))\n","        social_component = c2 * self._get_random() * (np.array(global_best_position) - np.array(current_position[1:]))\n","        new_velocity = inertia + cognitive_component + social_component\n","        return new_velocity.tolist()\n","\n","    def _update_position(self, current_position, velocity):\n","        new_position = np.array(current_position) + np.array([0] + velocity)\n","        return new_position.tolist()\n","\n","    @staticmethod\n","    def _get_random():\n","        return np.random.uniform(low=0, high=1)\n","\n","    @staticmethod\n","    def _get_random_in_range(min_val, max_val):\n","        return np.random.uniform(low=min_val, high=max_val)\n","\n","    @staticmethod\n","    def _clip(value, min_val, max_val):\n","        return max(min(value, max_val), min_val)\n","# Creating a Particle Swarm Optimizer object\n","pso = ParticleSwarmOptimizer(num_particles, objective_function, param_boundaries)\n","# Finding the optimal parameters\n","optimal_params = pso.optimize(num_iterations)\n","# Displaying the best parameters found\n","print(f\"Best parameters found by PSO: {optimal_params}\")\n","# Compiling the model with the best parameters\n","model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=optimal_params[0]),\n","              loss='categorical_crossentropy',\n","              metrics=['accuracy'])\n","# Training the model with the best parameters\n","history = model.fit(X_train, y_train, batch_size=int(optimal_params[1]), epochs=int(optimal_params[2]), verbose=1)\n","# Evaluating the model on the test set\n","test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n","print(f\"Test loss: {test_loss}, Test accuracy: {test_accuracy}\")"],"metadata":{"id":"IkTzwMr4oZGm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**14. Particle swarm Optimization using GELU Activation function**"],"metadata":{"id":"joSow-yHuoqW"}},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras.utils import to_categorical\n","import numpy as np\n","\n","# Define GELU activation function\n","def gelu(x):\n","    cdf = 0.5 * (1.0 + tf.tanh((tf.sqrt(2 / tf.constant(np.pi)) * (x + 0.044715 * x ** 3))))\n","    return x * cdf\n","\n","# Loading MNIST data set\n","mnist = tf.keras.datasets.mnist\n","(X_train, y_train), (X_test, y_test) = mnist.load_data()\n","\n","# Normalizing the pixel values to the range [0, 1]\n","X_train, X_test = X_train / 255.0, X_test / 255.0\n","\n","# Converting labels to categorical\n","y_train = to_categorical(y_train, 10)\n","y_test = to_categorical(y_test, 10)\n","\n","# CNN architecture with GELU Activation\n","model = Sequential()\n","model.add(Conv2D(32, (3, 3), padding='same', activation=gelu, input_shape=(28, 28, 1)))\n","model.add(MaxPooling2D(pool_size=(2, 2)))\n","model.add(Flatten())\n","model.add(Dense(128, activation=gelu))\n","model.add(Dense(10, activation='softmax'))\n","# Compiling the model\n","model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","# Define the objective function for Particle Swarm Optimization\n","def objective_function(params):\n","    learning_rate = params[0]\n","    batch_size = int(params[1])\n","    epochs = int(params[2])\n","\n","    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n","                  loss='categorical_crossentropy',\n","                  metrics=['accuracy'])\n","\n","    history = model.fit(X_train, y_train, validation_data=(X_test, y_test),\n","                        batch_size=batch_size, epochs=epochs, verbose=0)\n","\n","    train_loss = history.history['loss'][-1]\n","    train_accuracy = history.history['accuracy'][-1]\n","\n","    return train_loss + 1 - train_accuracy  # PSO minimizes the objective function, so maximize accuracy.\n","# Define the search space for PSO\n","param_boundaries = [\n","    (1e-6, 1e-4),  # Learning rate\n","    (32, 50),     # Batch size\n","    (5, 10)        # Epochs\n","]\n","num_particles= 50\n","# Number of iterations\n","num_iterations = 50\n","# Particle Swarm Optimization\n","class ParticleSwarmOptimizer:\n","    def __init__(self, num_particles, objective_function, param_boundaries):\n","        self.num_particles = num_particles\n","        self.objective_function = objective_function\n","        self.param_boundaries = param_boundaries\n","\n","    def optimize(self, num_iterations):\n","        # Initialization\n","        particles = [self._initialize_particle() for _ in range(self.num_particles)]\n","        global_best_position = particles[0][0]\n","        global_best_fitness = particles[0][-1]\n","\n","        for iteration in range(num_iterations):\n","            for i in range(self.num_particles):\n","                # Update velocity and position\n","                new_velocity = self._update_velocity(particles[i][1], particles[i][2], particles[i][3],\n","                                                     particles[i][0], global_best_position)\n","                new_position = self._update_position(particles[i][0], new_velocity)\n","\n","                # Ensure the search space boundaries\n","                new_position = [self._clip(val, *self.param_boundaries[idx]) for idx, val in enumerate(new_position)]\n","\n","                # Update fitness\n","                new_fitness = self.objective_function(new_position)\n","\n","                # Update personal best\n","                if new_fitness < particles[i][-1]:\n","                    particles[i] = [new_position, *new_velocity, new_fitness]\n","\n","                # Update global best\n","                if new_fitness < global_best_fitness:\n","                    global_best_position = new_position\n","                    global_best_fitness = new_fitness\n","\n","        # Returning the best solution found\n","        return global_best_position\n","\n","    def _initialize_particle(self):\n","        position = [self._get_random_in_range(*boundaries) for boundaries in self.param_boundaries]\n","        velocity = [self._get_random_in_range(-1, 1) for _ in range(len(self.param_boundaries))]\n","        fitness = self.objective_function(position)\n","        return [position, *velocity, fitness]\n","\n","    def _update_velocity(self, w, c1, c2, current_position, global_best_position):\n","        inertia = w * np.array(current_position[1:])\n","        cognitive_component = c1 * self._get_random() * (np.array(current_position[3:]) - np.array(current_position[1:]))\n","        social_component = c2 * self._get_random() * (np.array(global_best_position) - np.array(current_position[1:]))\n","        new_velocity = inertia + cognitive_component + social_component\n","        return new_velocity.tolist()\n","\n","    def _update_position(self, current_position, velocity):\n","        new_position = np.array(current_position) + np.array([0] + velocity)\n","        return new_position.tolist()\n","\n","    @staticmethod\n","    def _get_random():\n","        return np.random.uniform(low=0, high=1)\n","\n","    @staticmethod\n","    def _get_random_in_range(min_val, max_val):\n","        return np.random.uniform(low=min_val, high=max_val)\n","\n","    @staticmethod\n","    def _clip(value, min_val, max_val):\n","        return max(min(value, max_val), min_val)\n","# Creating a Particle Swarm Optimizer object\n","pso = ParticleSwarmOptimizer(num_particles, objective_function, param_boundaries)\n","# Finding the optimal parameters\n","optimal_params = pso.optimize(num_iterations)\n","# Displaying the best parameters found\n","print(f\"Best parameters found by PSO: {optimal_params}\")\n","# Compiling the model with the best parameters\n","model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=optimal_params[0]),\n","              loss='categorical_crossentropy',\n","              metrics=['accuracy'])\n","# Training the model with the best parameters\n","history = model.fit(X_train, y_train, batch_size=int(optimal_params[1]), epochs=int(optimal_params[2]), verbose=1)\n","# Evaluating the model on the test set\n","test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n","print(f\"Test loss: {test_loss}, Test accuracy: {test_accuracy}\")"],"metadata":{"id":"YKOfnqKtuwJm"},"execution_count":null,"outputs":[]}]}